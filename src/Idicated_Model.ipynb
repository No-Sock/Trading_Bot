{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5b8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def Write_to_file(Date, net_worth, filename='{}.txt'.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))):\n",
    "    for i in net_worth: \n",
    "        Date += \" {}\".format(i)\n",
    "    #print(Date)\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    file = open(\"logs/\"+filename, 'a+')\n",
    "    file.write(Date+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ffa3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "from datetime import datetime\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class TradingGraph:\n",
    "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
    "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
    "    # call render every step\n",
    "    def __init__(self, Render_range):\n",
    "        self.Volume = deque(maxlen=Render_range)\n",
    "        self.net_worth = deque(maxlen=Render_range)\n",
    "        self.render_data = deque(maxlen=Render_range)\n",
    "        self.Render_range = Render_range\n",
    "\n",
    "        # We are using the style ‘ggplot’\n",
    "        plt.style.use('ggplot')\n",
    "        # close all plots if there are open\n",
    "        plt.close('all')\n",
    "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
    "        self.fig = plt.figure(figsize=(16,8)) \n",
    "\n",
    "        # Create top subplot for price axis\n",
    "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
    "        \n",
    "        # Create bottom subplot for volume which shares its x-axis\n",
    "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
    "        \n",
    "        # Create a new axis for net worth which shares its x-axis with price\n",
    "        self.ax3 = self.ax1.twinx()\n",
    "\n",
    "        # Formatting Date\n",
    "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        #self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        \n",
    "        # Add paddings to make graph easier to view\n",
    "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
    "\n",
    "        # we need to set layers\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.set_ylabel('Balance')\n",
    "\n",
    "        # I use tight_layout to replace plt.subplots_adjust\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        # Show the graph with matplotlib\n",
    "        plt.show()\n",
    "        \n",
    "    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):\n",
    "        # append volume and net_worth to deque list\n",
    "        self.Volume.append(Volume)\n",
    "        self.net_worth.append(net_worth)\n",
    "\n",
    "        # before appending to deque list, need to convert Date to special format\n",
    "        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]\n",
    "        self.render_data.append([Date, Open, High, Low, Close])\n",
    "        \n",
    "        # Clear the frame rendered last step\n",
    "        self.ax1.clear()\n",
    "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)\n",
    "\n",
    "        # Put all dates to one list and fill ax2 sublot with volume\n",
    "        Date_Render_range = [i[0] for i in self.render_data]\n",
    "        self.ax2.clear()\n",
    "        self.ax2.fill_between(Date_Render_range, self.Volume, 0)\n",
    "\n",
    "        # draw our net_worth graph on ax3 (shared with ax1) subplot\n",
    "        self.ax3.clear()\n",
    "        self.ax3.plot(Date_Render_range, self.net_worth, color=\"blue\")\n",
    "        \n",
    "        # beautify the x-labels (Our Date format)\n",
    "        self.ax1.xaxis.set_major_formatter(self.date_format)\n",
    "        self.fig.autofmt_xdate()\n",
    "\n",
    "        # sort sell and buy orders, put arrows in appropiate order positions\n",
    "        for trade in trades:\n",
    "            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]\n",
    "            if trade_date in Date_Render_range:\n",
    "                if trade['type'] == 'buy':\n",
    "                    high_low = trade['Low']-10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker=\"^\")\n",
    "                else:\n",
    "                    high_low = trade['High']+10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker=\"v\")\n",
    "\n",
    "        # we need to set layers every step, because we are clearing subplots every step\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.set_ylabel('Balance')\n",
    "\n",
    "        # I use tight_layout to replace plt.subplots_adjust\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
    "        # Show the graph without blocking the rest of the program\n",
    "        #plt.show(block=False)\n",
    "        # Necessary to view frames before they are unrendered\n",
    "        #plt.pause(0.001)\n",
    "\n",
    "        \"\"\"Display image with OpenCV - no interruption\"\"\"\n",
    "        # redraw the canvas\n",
    "        self.fig.canvas.draw()\n",
    "        # convert canvas to image\n",
    "        img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "        img  = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "        # img is rgb, convert to opencv's default bgr\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # display image with OpenCV or any operation you like\n",
    "        cv2.imshow(\"Bitcoin trading bot\",image)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5960a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99225448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgent:\n",
    "    # A custom Bitcoin trading agent\n",
    "    def __init__(self, lookback_window_size=50, lr=0.00005, epochs=1, optimizer=Adam, batch_size=32, model=\"\"):\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.model = model\n",
    "        \n",
    "        # Action space from 0 to 3, 0 is hold, 1 is buy, 2 is sell\n",
    "        self.action_space = np.array([0, 1, 2])\n",
    "\n",
    "        # folder to save models\n",
    "        self.log_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")+\"_Crypto_trader\"\n",
    "        \n",
    "        # State size contains Market+Orders history for the last lookback_window_size steps\n",
    "        self.state_size = (lookback_window_size, 10)\n",
    "\n",
    "        # Neural Networks part bellow\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create shared Actor-Critic network model\n",
    "        self.Actor = self.Critic = Shared_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer, model=self.model)\n",
    "        # Create Actor-Critic network model\n",
    "        #self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "        #self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "    # create tensorboard writer\n",
    "    def create_writer(self, initial_balance, normalize_value, train_episodes):\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter('runs/'+self.log_name)\n",
    "\n",
    "        # Create folder to save models\n",
    "        if not os.path.exists(self.log_name):\n",
    "            os.makedirs(self.log_name)\n",
    "\n",
    "        self.start_training_log(initial_balance, normalize_value, train_episodes)\n",
    "            \n",
    "    def start_training_log(self, initial_balance, normalize_value, train_episodes):      \n",
    "        # save training parameters to Parameters.txt file for future\n",
    "        with open(self.log_name+\"/Parameters.txt\", \"w\") as params:\n",
    "            current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "            params.write(f\"training start: {current_date}\\n\")\n",
    "            params.write(f\"initial_balance: {initial_balance}\\n\")\n",
    "            params.write(f\"training episodes: {train_episodes}\\n\")\n",
    "            params.write(f\"lookback_window_size: {self.lookback_window_size}\\n\")\n",
    "            params.write(f\"lr: {self.lr}\\n\")\n",
    "            params.write(f\"epochs: {self.epochs}\\n\")\n",
    "            params.write(f\"batch size: {self.batch_size}\\n\")\n",
    "            params.write(f\"normalize_value: {normalize_value}\\n\")\n",
    "            params.write(f\"model: {self.model}\\n\")\n",
    "            \n",
    "    def end_training_log(self):\n",
    "        with open(self.log_name+\"/Parameters.txt\", \"a+\") as params:\n",
    "            current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "            params.write(f\"training end: {current_date}\\n\")\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.critic_predict(states)\n",
    "        next_values = self.Critic.critic_predict(next_states)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        plt.plot(target,'-')\n",
    "        plt.plot(advantages,'.')\n",
    "        ax=plt.gca()\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
    "        c_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    "\n",
    "        return np.sum(a_loss.history['loss']), np.sum(c_loss.history['loss'])\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.actor_predict(np.expand_dims(state, axis=0))[0]\n",
    "        action = np.random.choice(self.action_space, p=prediction)\n",
    "        return action, prediction\n",
    "        \n",
    "    def save(self, name=\"Crypto_trader\", score=\"\", args=[]):\n",
    "        # save keras model weights\n",
    "        self.Actor.Actor.save_weights(f\"{self.log_name}/{score}_{name}_Actor.h5\")\n",
    "        self.Critic.Critic.save_weights(f\"{self.log_name}/{score}_{name}_Critic.h5\")\n",
    "\n",
    "        # log saved model arguments to file\n",
    "        if len(args) > 0:\n",
    "            with open(f\"{self.log_name}/log.txt\", \"a+\") as log:\n",
    "                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                log.write(f\"{current_time}, {args[0]}, {args[1]}, {args[2]}, {args[3]}, {args[4]}\\n\")\n",
    "\n",
    "    def load(self, folder, name):\n",
    "        # load keras model weights\n",
    "        self.Actor.Actor.load_weights(os.path.join(folder, f\"{name}_Actor.h5\"))\n",
    "        self.Critic.Critic.load_weights(os.path.join(folder, f\"{name}_Critic.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464509ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "class CustomEnv:\n",
    "    # A custom Bitcoin trading environment\n",
    "    def __init__(self, df, initial_balance=1000, lookback_window_size=50, Render_range = 100):\n",
    "        # Define action space and state size and other custom parameters\n",
    "        self.df = df.dropna().reset_index()\n",
    "        self.df_total_steps = len(self.df)-1\n",
    "        self.initial_balance = initial_balance\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.Render_range = Render_range\n",
    "\n",
    "        # Action space from 0 to 3, 0 is hold, 1 is buy, 2 is sell\n",
    "        self.action_space = np.array([0, 1, 2])\n",
    "\n",
    "        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps\n",
    "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
    "        \n",
    "        # Market history contains the OHCL values for the last lookback_window_size prices\n",
    "        self.market_history = deque(maxlen=self.lookback_window_size)\n",
    "\n",
    "        # State size contains Market+Orders history for the last lookback_window_size steps\n",
    "        self.state_size = (self.lookback_window_size, 10)\n",
    "        # Neural Networks part bellow\n",
    "        self.lr = 0.0001\n",
    "        self.epochs = 1\n",
    "        self.normalize_value = 100000\n",
    "        self.optimizer = Adam\n",
    "        self\n",
    "        # Create Actor-Critic network model\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "\n",
    "    # Reset the state of the environment to an initial state\n",
    "    def reset(self, env_steps_size = 0):\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.prev_net_worth = self.initial_balance\n",
    "        self.crypto_held = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.crypto_bought = 0\n",
    "        self.episode_orders = 0 # test\n",
    "        if env_steps_size > 0: # used for training dataset\n",
    "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)\n",
    "            self.end_step = self.start_step + env_steps_size\n",
    "            self.episode_orders += 1\n",
    "        else: # used for testing dataset\n",
    "            self.start_step = self.lookback_window_size\n",
    "            self.end_step = self.df_total_steps\n",
    "            self.episode_orders += 1\n",
    "\n",
    "        self.current_step = self.start_step\n",
    "\n",
    "        for i in reversed(range(self.lookback_window_size)):\n",
    "            current_step = self.current_step - i\n",
    "            self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "            self.market_history.append([self.df.loc[current_step, 'open'],\n",
    "                                    self.df.loc[current_step, 'high'],\n",
    "                                    self.df.loc[current_step, 'low'],\n",
    "                                    self.df.loc[current_step, 'close'],\n",
    "                                    self.df.loc[current_step, 'volume']\n",
    "                                    ])\n",
    "\n",
    "        state = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
    "        return state\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        self.market_history.append([self.df.loc[self.current_step, 'open'],\n",
    "                                    self.df.loc[self.current_step, 'high'],\n",
    "                                    self.df.loc[self.current_step, 'low'],\n",
    "                                    self.df.loc[self.current_step, 'close'],\n",
    "                                    self.df.loc[self.current_step, 'volume']\n",
    "                                    ])\n",
    "        obs = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.crypto_bought = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Set the current price to a random price between open and close\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, 'open'],\n",
    "            self.df.loc[self.current_step, 'close'])\n",
    "\n",
    "        if action == 0: # Hold\n",
    "            pass\n",
    "\n",
    "        elif action == 1 and self.balance > 0:\n",
    "            # Buy with 100% of current balance\n",
    "            self.crypto_bought = self.balance / current_price\n",
    "            self.balance -= self.crypto_bought * current_price\n",
    "            self.crypto_held += self.crypto_bought\n",
    "\n",
    "        elif action == 2 and self.crypto_held>0:\n",
    "            # Sell 100% of current crypto held\n",
    "            self.crypto_sold = self.crypto_held\n",
    "            self.balance += self.crypto_sold * current_price\n",
    "            self.crypto_held -= self.crypto_sold\n",
    "\n",
    "        self.prev_net_worth = self.net_worth\n",
    "        self.net_worth = self.balance + self.crypto_held * current_price\n",
    "\n",
    "        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.net_worth - self.prev_net_worth\n",
    "\n",
    "        if self.net_worth <= self.initial_balance/2:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done\n",
    "    \n",
    "    def create_writer(self):\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"Trader\")\n",
    "\n",
    "    def render(self, visualize = False):\n",
    "        #print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')\n",
    "        if visualize:\n",
    "            Open = self.df.loc[self.current_step, 'open']\n",
    "            Close = self.df.loc[self.current_step, 'close']\n",
    "            High = self.df.loc[self.current_step, 'high']\n",
    "            Low = self.df.loc[self.current_step, 'low']\n",
    "            Volume = self.df.loc[self.current_step, 'volume']\n",
    "\n",
    "            # Render the environment to the screen\n",
    "            self.visualization.render(Date, Open, High, Low, Close, Volume, self.net_worth, self.trades)\n",
    "    \n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "    \n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        #discounted_r = np.vstack(self.discount_rewards(rewards))\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "        # Compute advantages\n",
    "        #advantages = discounted_r - values\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(target,'-')\n",
    "        pylab.plot(advantages,'.')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True)\n",
    "        c_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(np.expand_dims(state, axis=0))[0]\n",
    "        action = np.random.choice(self.action_space, p=prediction)\n",
    "        return action, prediction\n",
    "\n",
    "    def save(self, name=\"Crypto_trader\"):\n",
    "        # save keras model weights\n",
    "        self.Actor.Actor.save_weights(f\"{name}_Actor.h5\")\n",
    "        self.Critic.Critic.save_weights(f\"{name}_Critic.h5\")\n",
    "\n",
    "    def load(self, name=\"Crypto_trader\"):\n",
    "        # load keras model weights\n",
    "        self.Actor.Actor.load_weights(f\"{name}_Actor.h5\")\n",
    "        self.Critic.Critic.load_weights(f\"{name}_Critic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67fea414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "class Shared_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer, model=\"Dense\"):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Shared CNN layers:\n",
    "        if model==\"CNN\":\n",
    "            X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
    "            X = MaxPooling1D(pool_size=2)(X)\n",
    "            X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
    "            X = MaxPooling1D(pool_size=2)(X)\n",
    "            X = Flatten()(X)\n",
    "\n",
    "        # Shared LSTM layers:\n",
    "        elif model==\"LSTM\":\n",
    "            X = LSTM(512, return_sequences=True)(X_input)\n",
    "            X = LSTM(256)(X)\n",
    "\n",
    "        # Shared Dense layers:\n",
    "        else:\n",
    "            X = Flatten()(X_input)\n",
    "            X = Dense(512, activation=\"relu\")(X)\n",
    "        \n",
    "        # Critic model\n",
    "        V = Dense(512, activation=\"relu\")(X)\n",
    "        V = Dense(256, activation=\"relu\")(V)\n",
    "        V = Dense(64, activation=\"relu\")(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=X_input, outputs = value)\n",
    "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "        # Actor model\n",
    "        A = Dense(512, activation=\"relu\")(X)\n",
    "        A = Dense(256, activation=\"relu\")(A)\n",
    "        A = Dense(64, activation=\"relu\")(A)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(A)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "        #print(self.Actor.summary())\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def actor_predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "    def critic_PPO2_loss(self, y_true, y_pred):\n",
    "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "\n",
    "    def critic_predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "        \n",
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Flatten(input_shape=input_shape)(X_input)\n",
    "        X = Dense(512, activation=\"relu\")(X)\n",
    "        X = Dense(256, activation=\"relu\")(X)\n",
    "        X = Dense(64, activation=\"relu\")(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "        #print(self.Actor.summary)\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def actor_predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "        V = Flatten(input_shape=input_shape)(X_input)\n",
    "        V = Dense(512, activation=\"relu\")(V)\n",
    "        V = Dense(256, activation=\"relu\")(V)\n",
    "        V = Dense(64, activation=\"relu\")(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=X_input, outputs = value)\n",
    "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, y_true, y_pred):\n",
    "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "\n",
    "    def critic_predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d4fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ta.trend import SMAIndicator, macd, PSARIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.momentum import rsi\n",
    "\n",
    "def AddIndicators(df):\n",
    "    # Add Simple Moving Average (SMA) indicators\n",
    "    df[\"sma7\"] = SMAIndicator(close=df[\"Close\"], window=7, fillna=True).sma_indicator()\n",
    "    df[\"sma25\"] = SMAIndicator(close=df[\"Close\"], window=25, fillna=True).sma_indicator()\n",
    "    df[\"sma99\"] = SMAIndicator(close=df[\"Close\"], window=99, fillna=True).sma_indicator()\n",
    "    \n",
    "    # Add Moving Average Convergence Divergence (MACD) indicator\n",
    "    df[\"MACD\"] = macd(close=df[\"Close\"], window_slow=26, window_fast=12, fillna=True) # mazas\n",
    "\n",
    "    # Add Relative Strength Index (RSI) indicator\n",
    "    df[\"RSI\"] = rsi(close=df[\"Close\"], window=14, fillna=True) # mazas\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57c69092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, visualize=False, train_episodes = 50, training_batch_size=500):\n",
    "    agent.create_writer(env.initial_balance, env.normalize_value, train_episodes) # create TensorBoard writer\n",
    "    total_average = deque(maxlen=100) # save recent 100 episodes net worth\n",
    "    best_average = 0 # used to track best average net worth\n",
    "    for episode in range(train_episodes):\n",
    "        state = env.reset(env_steps_size = training_batch_size)\n",
    "\n",
    "        states, actions, rewards, predictions, dones, next_states = [], [], [], [], [], []\n",
    "        for t in range(training_batch_size):\n",
    "            env.render(visualize)\n",
    "            action, prediction = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            states.append(np.expand_dims(state, axis=0))\n",
    "            next_states.append(np.expand_dims(next_state, axis=0))\n",
    "            action_onehot = np.zeros(3)\n",
    "            action_onehot[action] = 1\n",
    "            actions.append(action_onehot)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            predictions.append(prediction)\n",
    "            state = next_state\n",
    "\n",
    "        a_loss, c_loss = agent.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "        total_average.append(env.net_worth)\n",
    "        average = np.average(total_average)\n",
    "        \n",
    "        agent.writer.add_scalar('Data/average net_worth', average, episode)\n",
    "        agent.writer.add_scalar('Data/episode_orders', env.episode_orders, episode)\n",
    "        \n",
    "        print(\"episode: {:<5} net worth {:<7.2f} average: {:<7.2f} orders: {}\".format(episode, env.net_worth, average, env.episode_orders))\n",
    "        if episode > len(total_average):\n",
    "            if best_average < average:\n",
    "                best_average = average\n",
    "                print(\"Saving model\")\n",
    "                agent.save(score=\"{:.2f}\".format(best_average), args=[episode, average, env.episode_orders, a_loss, c_loss])\n",
    "            agent.save()\n",
    "            \n",
    "    agent.end_training_log()\n",
    "\n",
    "def test_agent(env, agent, visualize=True, test_episodes=10, folder=\"\", name=\"Crypto_trader\", comment=\"\"):\n",
    "    agent.load(folder, name)\n",
    "    average_net_worth = 0\n",
    "    average_orders = 0\n",
    "    no_profit_episodes = 0\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render(visualize)\n",
    "            action, prediction = agent.act(state)\n",
    "            state, reward, done = env.step(action)\n",
    "            if env.current_step == env.end_step:\n",
    "                average_net_worth += env.net_worth\n",
    "                average_orders += env.episode_orders\n",
    "                if env.net_worth < env.initial_balance: no_profit_episodes += 1 # calculate episode count where we had negative profit through episode\n",
    "                print(\"episode: {:<5}, net_worth: {:<7.2f}, average_net_worth: {:<7.2f}, orders: {}\".format(episode, env.net_worth, average_net_worth/(episode+1), env.episode_orders))\n",
    "                break\n",
    "            \n",
    "    print(\"average {} episodes agent net_worth: {}, orders: {}\".format(test_episodes, average_net_worth/test_episodes, average_orders/test_episodes))\n",
    "    print(\"No profit episodes: {}\".format(no_profit_episodes))\n",
    "    # save test results to test_results.txt file\n",
    "    with open(\"test_results.txt\", \"a+\") as results:\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "        results.write(f'{current_date}, {name}, test episodes:{test_episodes}')\n",
    "        results.write(f', net worth:{average_net_worth/(episode+1)}, orders per episode:{average_orders/test_episodes}')\n",
    "        results.write(f', no profit episodes:{no_profit_episodes}, model: {agent.model}, comment: {comment}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fccb97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvDatafeed import TvDatafeed, Interval\n",
    "\n",
    "username = 'No_Sock'\n",
    "password = '#TradeCore$23'\n",
    "tv = TvDatafeed(username, password)\n",
    "cdpr_data = tv.get_hist(symbol='CDR',exchange='GPW',interval=Interval.in_1_hour,n_bars=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bad81713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cdpr_data[['open','high','low','close','volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c4ee545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-04 11:00:00</th>\n",
       "      <td>276.50</td>\n",
       "      <td>278.60</td>\n",
       "      <td>268.10</td>\n",
       "      <td>270.60</td>\n",
       "      <td>176070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 12:00:00</th>\n",
       "      <td>270.70</td>\n",
       "      <td>271.50</td>\n",
       "      <td>269.20</td>\n",
       "      <td>270.80</td>\n",
       "      <td>90729.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 13:00:00</th>\n",
       "      <td>270.80</td>\n",
       "      <td>275.90</td>\n",
       "      <td>270.50</td>\n",
       "      <td>275.40</td>\n",
       "      <td>47461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 14:00:00</th>\n",
       "      <td>275.20</td>\n",
       "      <td>276.00</td>\n",
       "      <td>274.00</td>\n",
       "      <td>274.60</td>\n",
       "      <td>48727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-04 15:00:00</th>\n",
       "      <td>275.10</td>\n",
       "      <td>275.60</td>\n",
       "      <td>272.10</td>\n",
       "      <td>275.30</td>\n",
       "      <td>42403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28 14:00:00</th>\n",
       "      <td>114.30</td>\n",
       "      <td>115.20</td>\n",
       "      <td>114.30</td>\n",
       "      <td>114.70</td>\n",
       "      <td>44941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28 15:00:00</th>\n",
       "      <td>114.70</td>\n",
       "      <td>114.70</td>\n",
       "      <td>113.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>18752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28 16:00:00</th>\n",
       "      <td>113.95</td>\n",
       "      <td>114.45</td>\n",
       "      <td>113.40</td>\n",
       "      <td>113.65</td>\n",
       "      <td>28878.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28 17:00:00</th>\n",
       "      <td>113.65</td>\n",
       "      <td>113.80</td>\n",
       "      <td>113.35</td>\n",
       "      <td>113.65</td>\n",
       "      <td>34551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28 18:00:00</th>\n",
       "      <td>113.30</td>\n",
       "      <td>113.30</td>\n",
       "      <td>113.30</td>\n",
       "      <td>113.30</td>\n",
       "      <td>26512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5247 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       open    high     low   close    volume\n",
       "datetime                                                     \n",
       "2021-01-04 11:00:00  276.50  278.60  268.10  270.60  176070.0\n",
       "2021-01-04 12:00:00  270.70  271.50  269.20  270.80   90729.0\n",
       "2021-01-04 13:00:00  270.80  275.90  270.50  275.40   47461.0\n",
       "2021-01-04 14:00:00  275.20  276.00  274.00  274.60   48727.0\n",
       "2021-01-04 15:00:00  275.10  275.60  272.10  275.30   42403.0\n",
       "...                     ...     ...     ...     ...       ...\n",
       "2023-04-28 14:00:00  114.30  115.20  114.30  114.70   44941.0\n",
       "2023-04-28 15:00:00  114.70  114.70  113.80  113.90   18752.0\n",
       "2023-04-28 16:00:00  113.95  114.45  113.40  113.65   28878.0\n",
       "2023-04-28 17:00:00  113.65  113.80  113.35  113.65   34551.0\n",
       "2023-04-28 18:00:00  113.30  113.30  113.30  113.30   26512.0\n",
       "\n",
       "[5247 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb5815ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPadX4\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ThinkPadX4\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ThinkPadX4\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0     net worth 1000.00 average: 1000.00 orders: 1\n",
      "episode: 1     net worth 1000.00 average: 1000.00 orders: 1\n",
      "episode: 2     net worth 1000.00 average: 1000.00 orders: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m CustomAgent(lookback_window_size\u001b[38;5;241m=\u001b[39mlookback_window_size, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mAdam, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      6\u001b[0m train_env \u001b[38;5;241m=\u001b[39m CustomEnv(train_df, lookback_window_size\u001b[38;5;241m=\u001b[39mlookback_window_size)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, visualize, train_episodes, training_batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[0;32m     21\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 23\u001b[0m a_loss, c_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m total_average\u001b[38;5;241m.\u001b[39mappend(env\u001b[38;5;241m.\u001b[39mnet_worth)\n\u001b[0;32m     25\u001b[0m average \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(total_average)\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mCustomAgent.replay\u001b[1;34m(self, states, actions, rewards, predictions, dones, next_states)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# training Actor and Critic networks\u001b[39;00m\n\u001b[0;32m     94\u001b[0m a_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mActor\u001b[38;5;241m.\u001b[39mActor\u001b[38;5;241m.\u001b[39mfit(states, y_true, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m---> 95\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/actor_loss_per_replay\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(a_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/critic_loss_per_replay\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(c_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:856\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    855\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    731\u001b[0m         )\n\u001b[0;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[0;32m    417\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[0;32m    418\u001b[0m )\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    423\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:4608\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4600\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4604\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[0;32m   4605\u001b[0m ):\n\u001b[0;32m   4606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4608\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[0;32m   4610\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4612\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4613\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4614\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1481\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1485\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lookback_window_size = 50\n",
    "test_window = 720 # 30 days \n",
    "train_df = df[:-test_window-lookback_window_size]\n",
    "test_df = df[-test_window-lookback_window_size:]\n",
    "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, optimizer=Adam, batch_size = 32)\n",
    "train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)\n",
    "train_agent(train_env, agent, visualize=False, train_episodes=5000, training_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fc8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b58f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
